{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1789b5bd",
   "metadata": {},
   "source": [
    "# Part 1 - Baseline Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a8c69c",
   "metadata": {},
   "source": [
    "## Naïve Bayes approach\n",
    "\n",
    "$$\\hat{c} = argmax_{c∈\\{0, 1\\}}\\biggl[\\sum \\limits_{i=1}^{k}log(\\theta_{c, w_{i}})\\biggr]$$where $\\theta_{c, w_{i}}=\\frac{n_{c,w}+\\alpha}{n_{c}+k\\alpha}$.\n",
    "\n",
    "Class likelihoods have been omitted due to the structure of the dataset (50/50 split between negative and positive sentiments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaac2d2",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e913ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7ed1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier:\n",
    "    def __init__(self, saved_weights=False):\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "\n",
    "        if saved_weights:\n",
    "            \"\"\"Loading saved weights for predictions\"\"\"\n",
    "            npz_files = np.load(saved_weights, allow_pickle=True)\n",
    "            self._vocabulary = npz_files['vocab'].tolist()\n",
    "            self._class_conditional_likelihoods = npz_files['weights']\n",
    "        else:\n",
    "            self._vocabulary = None\n",
    "            self._class_conditional_likelihoods = None\n",
    "\n",
    "        self._training_data = None\n",
    "        self._training_labels = None\n",
    "        self._bag_of_words = None\n",
    "\n",
    "    @staticmethod\n",
    "    def preProcessing(parsedData):\n",
    "        \"\"\"\n",
    "        Given a dataset containing labels followed by the review comment,\n",
    "        process the data into the right format - label turns into a boolean variable\n",
    "        and a comment is being filtered out of the stop words,\n",
    "        stemmed and parsed into a list of strings.\n",
    "\n",
    "        :param parsedData: a two-dimensional array of input data\n",
    "        :return: labels and processed data\n",
    "        \"\"\"\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        def filterStopWords(toFilter): return [word.lower() for word in toFilter if not word.lower() in stopWords]\n",
    "\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "        car_reviews = np.array(parsedData)\n",
    "\n",
    "        for row in car_reviews:\n",
    "            row[0] = 1 if row[0] == 'Pos' else 0\n",
    "            row[1] = ' '.join([stemmer.stem(word) for word in filterStopWords(word_tokenize(row[1]))])\n",
    "\n",
    "        return car_reviews[:, 0], car_reviews[:, 1]\n",
    "\n",
    "    def __vectorize(self, data):\n",
    "        \"\"\"\n",
    "        Vectorizing the text samples, saving the vocabulary and sorting it.\n",
    "\n",
    "        :return bag_of_words: fully-processed text data\n",
    "        \"\"\"\n",
    "\n",
    "        def preprocess(text):\n",
    "            \"\"\"Filtering out all the numbers and leftover dots\"\"\"\n",
    "            return re.sub(r'\\d+|[.]', '', text)\n",
    "\n",
    "        vectorizer = CountVectorizer(binary=True, max_features=750, vocabulary=self._vocabulary,\n",
    "                                     preprocessor=preprocess)\n",
    "        bag_of_words = vectorizer.fit_transform(data).toarray()\n",
    "        if self._vocabulary is None:\n",
    "            self._vocabulary = dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1]))\n",
    "\n",
    "        return bag_of_words\n",
    "\n",
    "    def __naiveBayes(self):\n",
    "        \"\"\"\n",
    "        Given a dataset with binary features (words), calculate the empirical\n",
    "        class-conditional likelihoods, that is, (P(w_i | c))\n",
    "        for all features w_i and both classes (c in {0, 1}).\n",
    "        Assume a multinomial feature distribution and use Laplace smoothing.\n",
    "        \"\"\"\n",
    "        # ALPHA parameter chosen via trial and error\n",
    "        ALPHA = 0.175\n",
    "        features = self._bag_of_words\n",
    "        labels = self._training_labels\n",
    "        feature_size = features.shape[1]\n",
    "        theta = np.empty((2, feature_size))\n",
    "\n",
    "        pos_features = []\n",
    "        neg_features = []\n",
    "        for i, row in enumerate(features):\n",
    "            pos_features.append(row) if labels[i] == 1 else neg_features.append(row)\n",
    "        pos_features = np.array(pos_features)\n",
    "        neg_features = np.array(neg_features)\n",
    "\n",
    "        for i, feature in enumerate(np.transpose(pos_features)):\n",
    "            theta[0][i] = (sum(feature) + ALPHA) / (feature_size * ALPHA + len(feature))\n",
    "\n",
    "        for i, feature in enumerate(np.transpose(neg_features)):\n",
    "            theta[1][i] = (sum(feature) + ALPHA) / (feature_size * ALPHA + len(feature))\n",
    "\n",
    "        self._class_conditional_likelihoods = theta\n",
    "\n",
    "    def __evaluate(self, data):\n",
    "        \"\"\"\n",
    "        Given a dataset with binary features, predict the corresponding\n",
    "        response for each instance (row) of the data set.\n",
    "\n",
    "        :param data: a two-dimensional numpy-array with shape = [n_test_samples, n_features].\n",
    "            theta[j, i] corresponds to the probability of feature i appearing in a sample belonging to class j.\n",
    "        :return class_predictions: a numpy array containing the class predictions for each row of data.\n",
    "        \"\"\"\n",
    "        class_predictions = np.empty(data.shape[0], dtype=np.int8)\n",
    "\n",
    "        for i, sample in enumerate(data):\n",
    "            p_feature_given_neg = np.empty(sample.shape)\n",
    "            p_feature_given_pos = np.empty(sample.shape)\n",
    "\n",
    "            for j, feature in enumerate(sample):\n",
    "                if feature == 0:\n",
    "                    p_feature_given_neg[j] = np.log2(1 - self._class_conditional_likelihoods[1][j])\n",
    "                    p_feature_given_pos[j] = np.log2(1 - self._class_conditional_likelihoods[0][j])\n",
    "                elif feature == 1:\n",
    "                    p_feature_given_neg[j] = np.log2(self._class_conditional_likelihoods[1][j])\n",
    "                    p_feature_given_pos[j] = np.log2(self._class_conditional_likelihoods[0][j])\n",
    "\n",
    "            neg_log_product = np.sum(p_feature_given_neg)\n",
    "            pos_log_product = np.sum(p_feature_given_pos)\n",
    "\n",
    "            class_predictions[i] = 1 if pos_log_product > neg_log_product else 0\n",
    "\n",
    "        return class_predictions\n",
    "\n",
    "    def train(self, data, save_weights=False):\n",
    "        \"\"\"\n",
    "        Training manager function - Data wrangling and training.\n",
    "\n",
    "        :param data: a two-dimensional array of input data\n",
    "        :param save_weights: flag indicator to save the current weights and vocabulary for future predictions\n",
    "        \"\"\"\n",
    "        self._training_labels, self._training_data = self.preProcessing(data)\n",
    "        self._bag_of_words = self.__vectorize(self._training_data)\n",
    "        self.__naiveBayes()\n",
    "        if save_weights:\n",
    "            np.savez_compressed('resources/weights_and_vocab.npz', weights=self._class_conditional_likelihoods, vocab=self._vocabulary)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Prediction manager function - Data wrangling and evaluating based on saved weights.\n",
    "\n",
    "        :param data: a two-dimensional array of input data\n",
    "        :return predictions: a vector of binary predictions\n",
    "        \"\"\"\n",
    "        labels, processed_data = self.preProcessing(data)\n",
    "        features = self.__vectorize(processed_data)\n",
    "        predictions = self.__evaluate(features)\n",
    "        conf_matrix = confusion_matrix(labels.astype(int), predictions)\n",
    "        training_set_accuracy = np.mean(np.equal(predictions, labels)) * 100\n",
    "\n",
    "        return conf_matrix, training_set_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fbf4d",
   "metadata": {},
   "source": [
    "## Code Flow Explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8804c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. parsing the CSV file\n",
    "parsedData = pd.read_csv('resources/car-reviews.csv')\n",
    "\n",
    "# 2. Test/Train split\n",
    "train_data, test_data = train_test_split(parsedData, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374e8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Creating the classifier object\n",
    "classifier = SentimentClassifier()\n",
    "\n",
    "# 4. Calling a train function (timing the operation)\n",
    "train_start_time = time.process_time()\n",
    "classifier.train(train_data, save_weights=True)\n",
    "train_end_time = time.process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311dfada",
   "metadata": {},
   "source": [
    "<pre><code><em><span style=\"color:darkcyan\"># 5. Input data goes into preprocessing</span></em>\n",
    "self._training_labels, self._training_data = \n",
    "    self.preProcessing(data)\n",
    "\n",
    "<em><span style=\"color:darkcyan\"># 5.1 Actual sentiments are written to a binary variable</span></em>\n",
    "<em><span style=\"color:darkcyan\"># 5.2 Reviews are being stemmed and filter out of stop words</span></em>\n",
    "for row in car_reviews:\n",
    "    row[0] = 1 if row[0] == 'Pos' else 0\n",
    "    row[1] = ' '.join([stemmer.stem(word)\n",
    "        for word in filterStopWords(word_tokenize(row[1]))])</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829c7e5",
   "metadata": {},
   "source": [
    "<pre><code><em><span style=\"color:darkcyan\"># 6. Input data gets vectorized</span></em>\n",
    "self._bag_of_words = self.__vectorize(self._training_data)\n",
    "\n",
    "<em><span style=\"color:darkcyan\"># 6.1 Fitting and transforming the reviews</span></em>\n",
    "bag_of_words = vectorizer.fit_transform(data).toarray()\n",
    "\n",
    "<em><span style=\"color:darkcyan\"># 6.2 Sorting the vocabulary (if not loaded from a file)</span></em>\n",
    "self._vocabulary = dict(sorted(vectorizer.vocabulary_.items(),\n",
    "    key=lambda item: item[1]))</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a8f7f",
   "metadata": {},
   "source": [
    "<pre><code><em><span style=\"color:darkcyan\"># 7. Initiating the Naive Bayes algorithm</span></em>\n",
    "self.__naiveBayes()\n",
    "\n",
    "<em><span style=\"color:darkcyan\"># 7.1 Splitting the sentiment features</span></em>\n",
    "for i, row in enumerate(features):\n",
    "    pos_features.append(row) if labels[i] is 1 \n",
    "        else neg_features.append(row)\n",
    "\n",
    "<em><span style=\"color:darkcyan\"># 7.2 Calculating theta of each feature for each sentiment - \n",
    "Using Laplace Smoothing technique in case of words that appear only in one class</span></em>\n",
    "for i, feature in enumerate(np.transpose(pos_features)):\n",
    "    theta[0][i] = (sum(feature) + ALPHA) / \n",
    "        (feature_size * ALPHA + len(feature))\n",
    "\n",
    "for i, feature in enumerate(np.transpose(neg_features)):\n",
    "    theta[1][i] = (sum(feature) + ALPHA) / \n",
    "        (feature_size * ALPHA + len(feature))</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a5a44",
   "metadata": {},
   "source": [
    "<pre><code><em><span style=\"color:darkcyan\"># 8. Saving the thetas and vocabulary to an .npz file - \n",
    "Vocabulary dictionary helps to deal with the new words that would appear in the test data. This way we stick to the single collection of words from the training process and use them to calculate the final prediction.</span></em>\n",
    "np.savez_compressed('resources/weights_and_vocab.npz', \n",
    "    weights=self._class_conditional_likelihoods, \n",
    "    vocab=self._vocabulary)</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63120aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Testing the algorithm using the testing data (timing the operation)\n",
    "test_start_time = time.process_time()\n",
    "conf_matrix, accuracy = classifier.predict(test_data)\n",
    "test_end_time = time.process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bef680",
   "metadata": {},
   "source": [
    "<pre><code><em><span style=\"color:darkcyan\"># 10. Input data goes into preprocessing (see step 5 for details)</span></em>\n",
    "labels, processed_data = self.preProcessing(data)</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b4b3d",
   "metadata": {},
   "source": [
    "<pre><code><em><span style=\"color:darkcyan\"># 11. Input data gets vectorized (see step 6 for details)</span></em>\n",
    "features = self.__vectorize(processed_data)</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a64fc",
   "metadata": {},
   "source": [
    "<pre><code><em><span style=\"color:darkcyan\"># 12. Calling the evaluate function</span></em>\n",
    "predictions = self.__evaluate(features)\n",
    "\n",
    "<em><span style=\"color:darkcyan\"># 12.1 Calculating the sum of all logs of theta for each sample based on the vectorized input</span></em>\n",
    "for j, feature in enumerate(sample):\n",
    "    if feature == 0:\n",
    "        p_feature_given_neg[j] = \n",
    "            np.log2(1 - self._class_conditional_likelihoods[1][j])\n",
    "        p_feature_given_pos[j] = \n",
    "            np.log2(1 - self._class_conditional_likelihoods[0][j])\n",
    "    elif feature == 1:\n",
    "        p_feature_given_neg[j] = \n",
    "            np.log2(self._class_conditional_likelihoods[1][j])\n",
    "        p_feature_given_pos[j] = \n",
    "            np.log2(self._class_conditional_likelihoods[0][j])\n",
    "neg_log_product = np.sum(p_feature_given_neg)\n",
    "pos_log_product = np.sum(p_feature_given_pos)\n",
    "\n",
    "<em><span style=\"color:darkcyan\"># 12.2 Evaluating which class is more likely given the log products</span></em>\n",
    "class_predictions[i] = \n",
    "    1 if pos_log_product > neg_log_product else 0\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479a585",
   "metadata": {},
   "source": [
    "<pre><code><em><span style=\"color:darkcyan\"># 13. Generating the confusion matrix and the accuracy, and returning the results</span></em>\n",
    "conf_matrix = confusion_matrix(labels.astype(int), predictions)\n",
    "training_set_accuracy = np.mean(np.equal(predictions, labels)) * 100\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150cb002",
   "metadata": {},
   "source": [
    "## Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ffeddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.5884%\n",
      "Training time: 3.7188s\n",
      "Testing time: 1.4062s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.4f}%\\n\"\n",
    "      f\"Training time: {train_end_time - train_start_time:.4f}s\\n\"\n",
    "      f\"Testing time: {test_end_time - test_start_time:.4f}s\")\n",
    "ax = sns.heatmap(conf_matrix, annot=True, cmap='Reds', fmt='g')\n",
    "ax.set_title('Confusion Matrix\\n')\n",
    "ax.set_xlabel('Predicted Values')\n",
    "ax.set_ylabel('True Values')\n",
    "ax.xaxis.set_ticklabels(['False', 'True'])\n",
    "ax.yaxis.set_ticklabels(['False', 'True'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff78de",
   "metadata": {},
   "source": [
    "## Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aea1ce",
   "metadata": {},
   "source": [
    "In this part, I will be pasting in the code snippets and executing them using makeshift examples to demonstrate that the assessment criteria are met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95c57a",
   "metadata": {},
   "source": [
    "#### Criteria 1, 2 and 3: Excluding the stop words and punctuation; Handling words  in case-insensitive way; Recognising words with the same stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "161f8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = \"\"\"\n",
    "123, Knight, knightly, knights, CONSIGN, consigned, consigning,\n",
    "kNiTtInG, KNit, knits, knitted, 6412. - ? ! \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b536c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  knight  knight  knight  consign  consign  consign  knit  knit  knit  knit      '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "def filterStopWords(toFilter): \n",
    "    return [word.lower() for word in toFilter if not word.lower() in stopWords]\n",
    "def preprocess(text): \n",
    "    return re.sub(r'\\d+|[.,-?!]', '', text)\n",
    "\n",
    "filtering_stop_words = filterStopWords(word_tokenize(example_string))\n",
    "stemming = ' '.join([stemmer.stem(word) for word in filtering_stop_words])\n",
    "preprocess(stemming)\n",
    "# NOTE: Excess whitespace is being dealt with automatically inside the vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b8fdd",
   "metadata": {},
   "source": [
    "#### Criterion 4: Producing a vector of each review containing a set of binary values indicating the presence of the word/stem in that review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f81bdef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_reviews = pd.read_csv('resources/car-reviews-demo.csv')\n",
    "demo_classifier = SentimentClassifier()\n",
    "demo_classifier.train(demo_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f34a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 750\n",
      "Bag of words shape: (10, 750)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {len(demo_classifier._vocabulary)}\\n\"\n",
    "      f\"Bag of words shape: {demo_classifier._bag_of_words.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9175f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_classifier._bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c3182fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ab': 0,\n",
       " 'abl': 1,\n",
       " 'abund': 2,\n",
       " 'acceler': 3,\n",
       " 'accid': 4,\n",
       " 'accler': 5,\n",
       " 'accord': 6,\n",
       " 'account': 7,\n",
       " 'activ': 8,\n",
       " 'actual': 9,\n",
       " 'actuat': 10,\n",
       " 'ad': 11,\n",
       " 'addit': 12,\n",
       " 'adjust': 13,\n",
       " 'admir': 14,\n",
       " 'air': 15,\n",
       " 'ajar': 16,\n",
       " 'allow': 17,\n",
       " 'almost': 18,\n",
       " 'along': 19,\n",
       " 'alot': 20,\n",
       " 'alreadi': 21,\n",
       " 'also': 22,\n",
       " 'although': 23,\n",
       " 'alway': 24,\n",
       " 'amount': 25,\n",
       " 'anoth': 26,\n",
       " 'anti': 27,\n",
       " 'anyon': 28,\n",
       " 'anyth': 29,\n",
       " 'anyway': 30,\n",
       " 'appear': 31,\n",
       " 'appli': 32,\n",
       " 'approxim': 33,\n",
       " 'area': 34,\n",
       " 'around': 35,\n",
       " 'asphalt': 36,\n",
       " 'assembl': 37,\n",
       " 'associ': 38,\n",
       " 'assum': 39,\n",
       " 'attach': 40,\n",
       " 'audio': 41,\n",
       " 'avoid': 42,\n",
       " 'axl': 43,\n",
       " 'back': 44,\n",
       " 'bad': 45,\n",
       " 'baddest': 46,\n",
       " 'bag': 47,\n",
       " 'balanc': 48,\n",
       " 'ball': 49,\n",
       " 'base': 50,\n",
       " 'basi': 51,\n",
       " 'batteri': 52,\n",
       " 'bed': 53,\n",
       " 'beep': 54,\n",
       " 'began': 55,\n",
       " 'behind': 56,\n",
       " 'belt': 57,\n",
       " 'bench': 58,\n",
       " 'best': 59,\n",
       " 'better': 60,\n",
       " 'big': 61,\n",
       " 'biggest': 62,\n",
       " 'bill': 63,\n",
       " 'birth': 64,\n",
       " 'bleach': 65,\n",
       " 'blind': 66,\n",
       " 'block': 67,\n",
       " 'bmw': 68,\n",
       " 'boat': 69,\n",
       " 'bone': 70,\n",
       " 'bottom': 71,\n",
       " 'bought': 72,\n",
       " 'brake': 73,\n",
       " 'breez': 74,\n",
       " 'bring': 75,\n",
       " 'broke': 76,\n",
       " 'broken': 77,\n",
       " 'brought': 78,\n",
       " 'bucket': 79,\n",
       " 'buckl': 80,\n",
       " 'built': 81,\n",
       " 'bumber': 82,\n",
       " 'bummer': 83,\n",
       " 'bump': 84,\n",
       " 'bumper': 85,\n",
       " 'busi': 86,\n",
       " 'buy': 87,\n",
       " 'cab': 88,\n",
       " 'cabin': 89,\n",
       " 'cake': 90,\n",
       " 'came': 91,\n",
       " 'camp': 92,\n",
       " 'cant': 93,\n",
       " 'capac': 94,\n",
       " 'car': 95,\n",
       " 'caravan': 96,\n",
       " 'card': 97,\n",
       " 'care': 98,\n",
       " 'cargo': 99,\n",
       " 'carpet': 100,\n",
       " 'carri': 101,\n",
       " 'carseat': 102,\n",
       " 'case': 103,\n",
       " 'caus': 104,\n",
       " 'cd': 105,\n",
       " 'center': 106,\n",
       " 'certain': 107,\n",
       " 'chanc': 108,\n",
       " 'chang': 109,\n",
       " 'changer': 110,\n",
       " 'cheap': 111,\n",
       " 'check': 112,\n",
       " 'chevrolet': 113,\n",
       " 'children': 114,\n",
       " 'chime': 115,\n",
       " 'choic': 116,\n",
       " 'chrome': 117,\n",
       " 'citi': 118,\n",
       " 'class': 119,\n",
       " 'clean': 120,\n",
       " 'close': 121,\n",
       " 'club': 122,\n",
       " 'coat': 123,\n",
       " 'code': 124,\n",
       " 'coin': 125,\n",
       " 'cold': 126,\n",
       " 'come': 127,\n",
       " 'comfort': 128,\n",
       " 'compar': 129,\n",
       " 'complain': 130,\n",
       " 'complaint': 131,\n",
       " 'condit': 132,\n",
       " 'consol': 133,\n",
       " 'constant': 134,\n",
       " 'contact': 135,\n",
       " 'contain': 136,\n",
       " 'continu': 137,\n",
       " 'control': 138,\n",
       " 'cool': 139,\n",
       " 'corner': 140,\n",
       " 'cost': 141,\n",
       " 'could': 142,\n",
       " 'coup': 143,\n",
       " 'cover': 144,\n",
       " 'coverag': 145,\n",
       " 'cramp': 146,\n",
       " 'creat': 147,\n",
       " 'critic': 148,\n",
       " 'cup': 149,\n",
       " 'current': 150,\n",
       " 'damag': 151,\n",
       " 'danger': 152,\n",
       " 'dash': 153,\n",
       " 'date': 154,\n",
       " 'day': 155,\n",
       " 'dead': 156,\n",
       " 'dealer': 157,\n",
       " 'dealership': 158,\n",
       " 'decent': 159,\n",
       " 'decid': 160,\n",
       " 'deduct': 161,\n",
       " 'deep': 162,\n",
       " 'defin': 163,\n",
       " 'deliv': 164,\n",
       " 'dent': 165,\n",
       " 'design': 166,\n",
       " 'desir': 167,\n",
       " 'despit': 168,\n",
       " 'diagnosi': 169,\n",
       " 'diagnost': 170,\n",
       " 'didnt': 171,\n",
       " 'diesel': 172,\n",
       " 'differ': 173,\n",
       " 'difficult': 174,\n",
       " 'difficulti': 175,\n",
       " 'ding': 176,\n",
       " 'direct': 177,\n",
       " 'discontinu': 178,\n",
       " 'disguis': 179,\n",
       " 'disk': 180,\n",
       " 'disput': 181,\n",
       " 'distract': 182,\n",
       " 'dodg': 183,\n",
       " 'doesnt': 184,\n",
       " 'done': 185,\n",
       " 'dont': 186,\n",
       " 'door': 187,\n",
       " 'downhil': 188,\n",
       " 'draw': 189,\n",
       " 'dream': 190,\n",
       " 'drive': 191,\n",
       " 'driven': 192,\n",
       " 'driver': 193,\n",
       " 'driveway': 194,\n",
       " 'drove': 195,\n",
       " 'dual': 196,\n",
       " 'due': 197,\n",
       " 'duti': 198,\n",
       " 'dwarf': 199,\n",
       " 'earlier': 200,\n",
       " 'eas': 201,\n",
       " 'easi': 202,\n",
       " 'ecu': 203,\n",
       " 'effect': 204,\n",
       " 'effici': 205,\n",
       " 'effort': 206,\n",
       " 'eight': 207,\n",
       " 'els': 208,\n",
       " 'end': 209,\n",
       " 'engin': 210,\n",
       " 'enjoy': 211,\n",
       " 'enough': 212,\n",
       " 'ensur': 213,\n",
       " 'entre': 214,\n",
       " 'entri': 215,\n",
       " 'escort': 216,\n",
       " 'especi': 217,\n",
       " 'estim': 218,\n",
       " 'etc': 219,\n",
       " 'evalu': 220,\n",
       " 'even': 221,\n",
       " 'everi': 222,\n",
       " 'everyon': 223,\n",
       " 'everyth': 224,\n",
       " 'exceed': 225,\n",
       " 'excel': 226,\n",
       " 'except': 227,\n",
       " 'excess': 228,\n",
       " 'excurs': 229,\n",
       " 'exist': 230,\n",
       " 'exit': 231,\n",
       " 'expect': 232,\n",
       " 'expens': 233,\n",
       " 'expert': 234,\n",
       " 'explor': 235,\n",
       " 'extend': 236,\n",
       " 'exterior': 237,\n",
       " 'extrem': 238,\n",
       " 'eyeglass': 239,\n",
       " 'fact': 240,\n",
       " 'factori': 241,\n",
       " 'fail': 242,\n",
       " 'fall': 243,\n",
       " 'famili': 244,\n",
       " 'fantast': 245,\n",
       " 'fare': 246,\n",
       " 'fast': 247,\n",
       " 'fear': 248,\n",
       " 'feast': 249,\n",
       " 'featur': 250,\n",
       " 'feedback': 251,\n",
       " 'feel': 252,\n",
       " 'feet': 253,\n",
       " 'fell': 254,\n",
       " 'felt': 255,\n",
       " 'find': 256,\n",
       " 'fine': 257,\n",
       " 'firm': 258,\n",
       " 'first': 259,\n",
       " 'fit': 260,\n",
       " 'fix': 261,\n",
       " 'flat': 262,\n",
       " 'focus': 263,\n",
       " 'fold': 264,\n",
       " 'foot': 265,\n",
       " 'ford': 266,\n",
       " 'forward': 267,\n",
       " 'found': 268,\n",
       " 'four': 269,\n",
       " 'frame': 270,\n",
       " 'francisco': 271,\n",
       " 'friend': 272,\n",
       " 'frighten': 273,\n",
       " 'front': 274,\n",
       " 'fuel': 275,\n",
       " 'full': 276,\n",
       " 'garag': 277,\n",
       " 'gas': 278,\n",
       " 'general': 279,\n",
       " 'get': 280,\n",
       " 'giant': 281,\n",
       " 'gigant': 282,\n",
       " 'give': 283,\n",
       " 'glare': 284,\n",
       " 'go': 285,\n",
       " 'good': 286,\n",
       " 'got': 287,\n",
       " 'great': 288,\n",
       " 'half': 289,\n",
       " 'handl': 290,\n",
       " 'happen': 291,\n",
       " 'hard': 292,\n",
       " 'hardwar': 293,\n",
       " 'heard': 294,\n",
       " 'heat': 295,\n",
       " 'height': 296,\n",
       " 'help': 297,\n",
       " 'hertz': 298,\n",
       " 'hesit': 299,\n",
       " 'hidden': 300,\n",
       " 'hide': 301,\n",
       " 'high': 302,\n",
       " 'highway': 303,\n",
       " 'hilli': 304,\n",
       " 'hitch': 305,\n",
       " 'hog': 306,\n",
       " 'hold': 307,\n",
       " 'holder': 308,\n",
       " 'hood': 309,\n",
       " 'hook': 310,\n",
       " 'hope': 311,\n",
       " 'horribl': 312,\n",
       " 'huge': 313,\n",
       " 'husband': 314,\n",
       " 'id': 315,\n",
       " 'ignit': 316,\n",
       " 'im': 317,\n",
       " 'impact': 318,\n",
       " 'import': 319,\n",
       " 'imposs': 320,\n",
       " 'inappropri': 321,\n",
       " 'includ': 322,\n",
       " 'increas': 323,\n",
       " 'indic': 324,\n",
       " 'individu': 325,\n",
       " 'inform': 326,\n",
       " 'infront': 327,\n",
       " 'initi': 328,\n",
       " 'inner': 329,\n",
       " 'insid': 330,\n",
       " 'inspect': 331,\n",
       " 'instal': 332,\n",
       " 'intend': 333,\n",
       " 'interior': 334,\n",
       " 'isnt': 335,\n",
       " 'issu': 336,\n",
       " 'item': 337,\n",
       " 'ive': 338,\n",
       " 'jack': 339,\n",
       " 'jam': 340,\n",
       " 'jar': 341,\n",
       " 'joint': 342,\n",
       " 'judg': 343,\n",
       " 'junk': 344,\n",
       " 'keep': 345,\n",
       " 'key': 346,\n",
       " 'kid': 347,\n",
       " 'kill': 348,\n",
       " 'kit': 349,\n",
       " 'lamp': 350,\n",
       " 'lancer': 351,\n",
       " 'lane': 352,\n",
       " 'larg': 353,\n",
       " 'last': 354,\n",
       " 'later': 355,\n",
       " 'leak': 356,\n",
       " 'least': 357,\n",
       " 'leather': 358,\n",
       " 'leav': 359,\n",
       " 'left': 360,\n",
       " 'less': 361,\n",
       " 'lie': 362,\n",
       " 'light': 363,\n",
       " 'like': 364,\n",
       " 'limit': 365,\n",
       " 'liner': 366,\n",
       " 'list': 367,\n",
       " 'liter': 368,\n",
       " 'littl': 369,\n",
       " 'lock': 370,\n",
       " 'logic': 371,\n",
       " 'long': 372,\n",
       " 'longer': 373,\n",
       " 'look': 374,\n",
       " 'loos': 375,\n",
       " 'loss': 376,\n",
       " 'lost': 377,\n",
       " 'lot': 378,\n",
       " 'love': 379,\n",
       " 'low': 380,\n",
       " 'lower': 381,\n",
       " 'lucki': 382,\n",
       " 'luggag': 383,\n",
       " 'luxuri': 384,\n",
       " 'machin': 385,\n",
       " 'made': 386,\n",
       " 'mainten': 387,\n",
       " 'make': 388,\n",
       " 'manag': 389,\n",
       " 'maneuv': 390,\n",
       " 'mani': 391,\n",
       " 'manufactur': 392,\n",
       " 'market': 393,\n",
       " 'massiv': 394,\n",
       " 'mate': 395,\n",
       " 'materi': 396,\n",
       " 'matter': 397,\n",
       " 'may': 398,\n",
       " 'mean': 399,\n",
       " 'meant': 400,\n",
       " 'mechan': 401,\n",
       " 'medal': 402,\n",
       " 'meet': 403,\n",
       " 'metal': 404,\n",
       " 'meticul': 405,\n",
       " 'might': 406,\n",
       " 'mildew': 407,\n",
       " 'mile': 408,\n",
       " 'mileag': 409,\n",
       " 'minder': 410,\n",
       " 'mini': 411,\n",
       " 'minim': 412,\n",
       " 'minimum': 413,\n",
       " 'minivan': 414,\n",
       " 'mistak': 415,\n",
       " 'model': 416,\n",
       " 'modul': 417,\n",
       " 'mold': 418,\n",
       " 'money': 419,\n",
       " 'month': 420,\n",
       " 'mother': 421,\n",
       " 'motor': 422,\n",
       " 'mount': 423,\n",
       " 'movement': 424,\n",
       " 'mpg': 425,\n",
       " 'much': 426,\n",
       " 'near': 427,\n",
       " 'need': 428,\n",
       " 'never': 429,\n",
       " 'new': 430,\n",
       " 'newer': 431,\n",
       " 'next': 432,\n",
       " 'nice': 433,\n",
       " 'nine': 434,\n",
       " 'normal': 435,\n",
       " 'noth': 436,\n",
       " 'notic': 437,\n",
       " 'object': 438,\n",
       " 'occas': 439,\n",
       " 'occurr': 440,\n",
       " 'offer': 441,\n",
       " 'often': 442,\n",
       " 'old': 443,\n",
       " 'one': 444,\n",
       " 'open': 445,\n",
       " 'opinion': 446,\n",
       " 'option': 447,\n",
       " 'outer': 448,\n",
       " 'outfit': 449,\n",
       " 'outsid': 450,\n",
       " 'overhead': 451,\n",
       " 'own': 452,\n",
       " 'owner': 453,\n",
       " 'pack': 454,\n",
       " 'packag': 455,\n",
       " 'paid': 456,\n",
       " 'pant': 457,\n",
       " 'park': 458,\n",
       " 'part': 459,\n",
       " 'pass': 460,\n",
       " 'passeng': 461,\n",
       " 'path': 462,\n",
       " 'pavement': 463,\n",
       " 'pay': 464,\n",
       " 'pedal': 465,\n",
       " 'peopl': 466,\n",
       " 'per': 467,\n",
       " 'perform': 468,\n",
       " 'pet': 469,\n",
       " 'petal': 470,\n",
       " 'pick': 471,\n",
       " 'plan': 472,\n",
       " 'pleasant': 473,\n",
       " 'plenti': 474,\n",
       " 'plug': 475,\n",
       " 'plus': 476,\n",
       " 'pocket': 477,\n",
       " 'point': 478,\n",
       " 'poor': 479,\n",
       " 'post': 480,\n",
       " 'pound': 481,\n",
       " 'power': 482,\n",
       " 'present': 483,\n",
       " 'press': 484,\n",
       " 'prevent': 485,\n",
       " 'price': 486,\n",
       " 'privledg': 487,\n",
       " 'probabl': 488,\n",
       " 'problem': 489,\n",
       " 'proclaim': 490,\n",
       " 'profession': 491,\n",
       " 'protect': 492,\n",
       " 'provid': 493,\n",
       " 'pull': 494,\n",
       " 'pulley': 495,\n",
       " 'pump': 496,\n",
       " 'purchas': 497,\n",
       " 'put': 498,\n",
       " 'quick': 499,\n",
       " 'quiet': 500,\n",
       " 'quit': 501,\n",
       " 'race': 502,\n",
       " 'radius': 503,\n",
       " 'rate': 504,\n",
       " 'rather': 505,\n",
       " 'read': 506,\n",
       " 'realiz': 507,\n",
       " 'realli': 508,\n",
       " 'rear': 509,\n",
       " 'rebuilt': 510,\n",
       " 'recommend': 511,\n",
       " 'refer': 512,\n",
       " 'refus': 513,\n",
       " 'regard': 514,\n",
       " 'relat': 515,\n",
       " 'remind': 516,\n",
       " 'remov': 517,\n",
       " 'rent': 518,\n",
       " 'rental': 519,\n",
       " 'repair': 520,\n",
       " 'replac': 521,\n",
       " 'requir': 522,\n",
       " 'resembl': 523,\n",
       " 'revers': 524,\n",
       " 'ride': 525,\n",
       " 'right': 526,\n",
       " 'rim': 527,\n",
       " 'road': 528,\n",
       " 'rod': 529,\n",
       " 'rode': 530,\n",
       " 'roll': 531,\n",
       " 'roller': 532,\n",
       " 'room': 533,\n",
       " 'roomi': 534,\n",
       " 'row': 535,\n",
       " 'run': 536,\n",
       " 'safe': 537,\n",
       " 'safeti': 538,\n",
       " 'sag': 539,\n",
       " 'said': 540,\n",
       " 'san': 541,\n",
       " 'saturn': 542,\n",
       " 'say': 543,\n",
       " 'scratch': 544,\n",
       " 'seat': 545,\n",
       " 'second': 546,\n",
       " 'section': 547,\n",
       " 'sedan': 548,\n",
       " 'see': 549,\n",
       " 'seem': 550,\n",
       " 'seen': 551,\n",
       " 'sens': 552,\n",
       " 'sept': 553,\n",
       " 'serpentin': 554,\n",
       " 'set': 555,\n",
       " 'seven': 556,\n",
       " 'sever': 557,\n",
       " 'shear': 558,\n",
       " 'shock': 559,\n",
       " 'shop': 560,\n",
       " 'side': 561,\n",
       " 'sight': 562,\n",
       " 'signific': 563,\n",
       " 'simpli': 564,\n",
       " 'sinc': 565,\n",
       " 'sit': 566,\n",
       " 'situat': 567,\n",
       " 'six': 568,\n",
       " 'size': 569,\n",
       " 'slide': 570,\n",
       " 'slow': 571,\n",
       " 'slung': 572,\n",
       " 'small': 573,\n",
       " 'smaller': 574,\n",
       " 'smooth': 575,\n",
       " 'snack': 576,\n",
       " 'snow': 577,\n",
       " 'solid': 578,\n",
       " 'solut': 579,\n",
       " 'someth': 580,\n",
       " 'sonar': 581,\n",
       " 'soon': 582,\n",
       " 'sound': 583,\n",
       " 'space': 584,\n",
       " 'spare': 585,\n",
       " 'spent': 586,\n",
       " 'sport': 587,\n",
       " 'spot': 588,\n",
       " 'spray': 589,\n",
       " 'spring': 590,\n",
       " 'squeak': 591,\n",
       " 'squeek': 592,\n",
       " 'stand': 593,\n",
       " 'standard': 594,\n",
       " 'start': 595,\n",
       " 'station': 596,\n",
       " 'stay': 597,\n",
       " 'steer': 598,\n",
       " 'step': 599,\n",
       " 'stereo': 600,\n",
       " 'stick': 601,\n",
       " 'still': 602,\n",
       " 'stop': 603,\n",
       " 'store': 604,\n",
       " 'strain': 605,\n",
       " 'strand': 606,\n",
       " 'strip': 607,\n",
       " 'strong': 608,\n",
       " 'style': 609,\n",
       " 'submarin': 610,\n",
       " 'suburban': 611,\n",
       " 'sudden': 612,\n",
       " 'suggest': 613,\n",
       " 'summertim': 614,\n",
       " 'sun': 615,\n",
       " 'sure': 616,\n",
       " 'surpris': 617,\n",
       " 'suspens': 618,\n",
       " 'suv': 619,\n",
       " 'sway': 620,\n",
       " 'swing': 621,\n",
       " 'switch': 622,\n",
       " 'sx': 623,\n",
       " 'system': 624,\n",
       " 'tag': 625,\n",
       " 'tailgat': 626,\n",
       " 'take': 627,\n",
       " 'taken': 628,\n",
       " 'tall': 629,\n",
       " 'tank': 630,\n",
       " 'tape': 631,\n",
       " 'taurus': 632,\n",
       " 'tendenc': 633,\n",
       " 'terribl': 634,\n",
       " 'test': 635,\n",
       " 'thank': 636,\n",
       " 'thee': 637,\n",
       " 'there': 638,\n",
       " 'thin': 639,\n",
       " 'thing': 640,\n",
       " 'think': 641,\n",
       " 'third': 642,\n",
       " 'thorough': 643,\n",
       " 'though': 644,\n",
       " 'thought': 645,\n",
       " 'three': 646,\n",
       " 'tie': 647,\n",
       " 'tight': 648,\n",
       " 'till': 649,\n",
       " 'time': 650,\n",
       " 'tini': 651,\n",
       " 'tip': 652,\n",
       " 'tire': 653,\n",
       " 'today': 654,\n",
       " 'ton': 655,\n",
       " 'took': 656,\n",
       " 'top': 657,\n",
       " 'torqu': 658,\n",
       " 'touch': 659,\n",
       " 'tough': 660,\n",
       " 'tow': 661,\n",
       " 'toward': 662,\n",
       " 'traction': 663,\n",
       " 'trade': 664,\n",
       " 'trailer': 665,\n",
       " 'tranni': 666,\n",
       " 'transfer': 667,\n",
       " 'transmiss': 668,\n",
       " 'transport': 669,\n",
       " 'treat': 670,\n",
       " 'tri': 671,\n",
       " 'trial': 672,\n",
       " 'trip': 673,\n",
       " 'truck': 674,\n",
       " 'trunk': 675,\n",
       " 'tune': 676,\n",
       " 'turbo': 677,\n",
       " 'turn': 678,\n",
       " 'twice': 679,\n",
       " 'twisti': 680,\n",
       " 'two': 681,\n",
       " 'unavail': 682,\n",
       " 'uncomfort': 683,\n",
       " 'uneven': 684,\n",
       " 'unhook': 685,\n",
       " 'unknown': 686,\n",
       " 'unnecessarili': 687,\n",
       " 'upset': 688,\n",
       " 'us': 689,\n",
       " 'usag': 690,\n",
       " 'use': 691,\n",
       " 'useless': 692,\n",
       " 'usual': 693,\n",
       " 'util': 694,\n",
       " 'valu': 695,\n",
       " 'valv': 696,\n",
       " 'van': 697,\n",
       " 'vehicl': 698,\n",
       " 'vent': 699,\n",
       " 'version': 700,\n",
       " 'vibrat': 701,\n",
       " 'visibl': 702,\n",
       " 'vision': 703,\n",
       " 'visor': 704,\n",
       " 'wagon': 705,\n",
       " 'wait': 706,\n",
       " 'wall': 707,\n",
       " 'wallet': 708,\n",
       " 'want': 709,\n",
       " 'warmer': 710,\n",
       " 'warn': 711,\n",
       " 'warranti': 712,\n",
       " 'wasnt': 713,\n",
       " 'way': 714,\n",
       " 'wd': 715,\n",
       " 'week': 716,\n",
       " 'weigh': 717,\n",
       " 'weight': 718,\n",
       " 'welcom': 719,\n",
       " 'well': 720,\n",
       " 'went': 721,\n",
       " 'werent': 722,\n",
       " 'wet': 723,\n",
       " 'whatev': 724,\n",
       " 'wheel': 725,\n",
       " 'whether': 726,\n",
       " 'wide': 727,\n",
       " 'wind': 728,\n",
       " 'window': 729,\n",
       " 'windshield': 730,\n",
       " 'windstar': 731,\n",
       " 'winstar': 732,\n",
       " 'wiper': 733,\n",
       " 'wish': 734,\n",
       " 'within': 735,\n",
       " 'without': 736,\n",
       " 'wonder': 737,\n",
       " 'wont': 738,\n",
       " 'wood': 739,\n",
       " 'work': 740,\n",
       " 'worri': 741,\n",
       " 'worth': 742,\n",
       " 'would': 743,\n",
       " 'write': 744,\n",
       " 'wrong': 745,\n",
       " 'xlt': 746,\n",
       " 'yard': 747,\n",
       " 'year': 748,\n",
       " 'yet': 749}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_classifier._vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4d563",
   "metadata": {},
   "source": [
    "#### Criterion 5: Implementing the correct Naive Bayes approach for the given classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc49f0",
   "metadata": {},
   "source": [
    "State-of-the-art Naive Bayes classifier approach (according to Wikipedia) is as follows:\n",
    "\n",
    "$$\\hat{y} = argmax_{k∈\\{1,...,K\\}} p(C_{k})\\prod \\limits_{i=1}^{n}p(x_{i} | C_{k})$$ (Wikipedia, 2022)\n",
    "\n",
    "Let's compare it side-by-side with the current approach (and normalize the style and variables):\n",
    "\n",
    "$$\\hat{y} = argmax_{k∈\\{0, 1\\}}\\sum \\limits_{i=1}^{n}log(p(x_{i} | C_{k}))$$where $k$ is a class (sentiment), $i$ is the number of features, $x_{i}$ represents a single feature and $C_{k}$ is a probability of the class' (sentiment's) occurrence.<br />\n",
    "(Note that $\\theta$ in the equation on the top of the notebook, corresponds to the conditional probability of the feature occurring given the probability of the class, ie. $\\theta_{k, x_{i}} = p(x_{i} | C_{k})$\n",
    "\n",
    "First, let's note the absence of $p(C_{k})$ in the proposed solution. The sole reason for this is the fact that the data that had been given contains 50% of positive reviews and 50% of negative reviews, therefore rendering this operation unnecessary and negligible. Even the variation generated by the random train/test split is small enough to allow us to bypass this step and save some precious runtime.<br /><br />\n",
    "Secondly, I have applied logarithmic normalization to the solution. That is, each $\\theta$ is being converted to $log(\\theta)$ upon evaluation, which allows for a more stable algorithm by extending the working range of values from $(0, 1)$ to $(-\\infty, 0)$ without twisting the orders in which values occur. Due to that, the $\\prod$ sign is swapped for $\\sum$.<br /><br />\n",
    "In the code below, please note that the <code>\\_\\_naiveBayes</code> function calculates all $\\theta$s (applying the Laplace Smoothing) and the <code>\\_\\_evaluate</code> function reads the right values of $\\theta$ for each sample, calculates the sum of all $\\theta$s for both classes and returns the class that has the highest value of $\\theta$.<br />\n",
    "\n",
    "<pre><code>    \n",
    "def __naiveBayes(self):\n",
    "    # ALPHA parameter chosen via trial and error\n",
    "    ALPHA = 0.175\n",
    "    features = self._bag_of_words\n",
    "    labels = self._training_labels\n",
    "    feature_size = features.shape[1]\n",
    "    theta = np.empty((2, feature_size))\n",
    "\n",
    "    pos_features = []\n",
    "    neg_features = []\n",
    "    for i, row in enumerate(features):\n",
    "        pos_features.append(row) if labels[i] == 1 else neg_features.append(row)\n",
    "    pos_features = np.array(pos_features)\n",
    "    neg_features = np.array(neg_features)\n",
    "\n",
    "    for i, feature in enumerate(np.transpose(pos_features)):\n",
    "        theta[0][i] = (sum(feature) + ALPHA) / (feature_size * ALPHA + len(feature))\n",
    "\n",
    "    for i, feature in enumerate(np.transpose(neg_features)):\n",
    "        theta[1][i] = (sum(feature) + ALPHA) / (feature_size * ALPHA + len(feature))\n",
    "\n",
    "    self._class_conditional_likelihoods = theta\n",
    "    \n",
    "def __evaluate(self, data):\n",
    "    class_predictions = np.empty(data.shape[0], dtype=np.int8)\n",
    "\n",
    "    for i, sample in enumerate(data):\n",
    "        p_feature_given_neg = np.empty(sample.shape)\n",
    "        p_feature_given_pos = np.empty(sample.shape)\n",
    "\n",
    "        for j, feature in enumerate(sample):\n",
    "            if feature == 0:\n",
    "                p_feature_given_neg[j] = \n",
    "                    np.log2(1 - self._class_conditional_likelihoods[1][j])\n",
    "                p_feature_given_pos[j] = \n",
    "                    np.log2(1 - self._class_conditional_likelihoods[0][j])\n",
    "            elif feature == 1:\n",
    "                p_feature_given_neg[j] = \n",
    "                    np.log2(self._class_conditional_likelihoods[1][j])\n",
    "                p_feature_given_pos[j] = \n",
    "                    np.log2(self._class_conditional_likelihoods[0][j])\n",
    "\n",
    "        neg_log_product = np.sum(p_feature_given_neg)\n",
    "        pos_log_product = np.sum(p_feature_given_pos)\n",
    "\n",
    "        class_predictions[i] = 1 if pos_log_product > neg_log_product else 0\n",
    "\n",
    "    return class_predictions\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f67b8d",
   "metadata": {},
   "source": [
    "#### Criterion 6: Using 80/20 training/testing data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0b80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8, Test size: 2\n",
      "Train size: 1105, Test size: 277, Split Ratio: 0.7995658465991317\n"
     ]
    }
   ],
   "source": [
    "demo_train, demo_test = train_test_split(demo_reviews, test_size=0.2)\n",
    "print(f\"Train size: {len(demo_train)}, Test size: {len(demo_test)}\")\n",
    "print(f\"Train size: {len(train_data)}, Test size: {len(test_data)}, \" \n",
    "      f\"Split Ratio: {len(train_data) / (len(train_data) + len(test_data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2e769",
   "metadata": {},
   "source": [
    "#### Criterion 7: Using ONLY training data during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb34a4",
   "metadata": {},
   "source": [
    "Looking back at the final score section, notice that <code>test_data</code> variable has not been used since the training/testing split.<br />The <code>train</code> function call in code cell number 4 (<code>classifier.train(train_data, save_weights=True</code>), triggers the whole process that takes place in a separate class, therefore it has no access to the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8cca7",
   "metadata": {},
   "source": [
    "#### Criterion 8: Handling of unexpected words that don't exist in the saved vocabulary dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c41205",
   "metadata": {},
   "source": [
    "Handling of unexpected words has been dealt with by simply saving the vocabulary to a file after an initial training and loading it as a reference to what stems/words the vectorizer should be looking out for.<br /><br />\n",
    "In short, any unexpected words, or even uncommon words that appeared in the testing sample, are being discarded.<br /><br />\n",
    "Let's look at the <code>\\_\\_vectorize</code> function and a snippet from the <code>\\_\\_init\\_\\_</code> function<br />\n",
    "\n",
    "<pre><code>\n",
    "    if saved_weights:\n",
    "        \"\"\"Loading saved weights for predictions\"\"\"\n",
    "        npz_files = np.load(saved_weights, allow_pickle=True)\n",
    "        self._vocabulary = npz_files['vocab'].tolist()\n",
    "        self._class_conditional_likelihoods = npz_files['weights']\n",
    "    else:\n",
    "        self._vocabulary = None\n",
    "        self._class_conditional_likelihoods = None\n",
    "</code></pre>\n",
    "\n",
    "<pre><code>\n",
    "def __vectorize(self, data):\n",
    "    def preprocess(text): return re.sub(r'\\d+|[.,-?!]', '', text)\n",
    "\n",
    "    vectorizer = CountVectorizer(binary=True, max_features=750, \n",
    "        vocabulary=self._vocabulary, preprocessor=preprocess)\n",
    "    bag_of_words = vectorizer.fit_transform(data).toarray()\n",
    "    if self._vocabulary is None:\n",
    "        self._vocabulary = dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1]))\n",
    "\n",
    "    return bag_of_words\n",
    "</code></pre>\n",
    "\n",
    "<br />Notice the <code>self._vocabulary</code> conditional assignment.<br />\n",
    "If we're training the new classifier, we would not be loading any existing weights, therefore <code>self._vocabulary</code> would be None.<br />\n",
    "In <code>CountVectorizer</code> function, None is the default variable for the <code>vocabulary</code> parameter, meaning the vectorizer would (in this case) choose the most occurring 750 words/stems and use them to create the bag of words dataset.<br />\n",
    "By saving these words/stems, we can reuse the same vocabulary during our predictions and even call the new classifier instance, load the weights and still carry the same vocabulary through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ec741",
   "metadata": {},
   "source": [
    "#### Criterion 9: Outputting the confusion matrix for performance demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66417902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As seen above\n",
    "ax = sns.heatmap(conf_matrix, annot=True, cmap='Reds', fmt='g')\n",
    "ax.set_title('Confusion Matrix\\n')\n",
    "ax.set_xlabel('Predicted Values')\n",
    "ax.set_ylabel('True Values')\n",
    "ax.xaxis.set_ticklabels(['False', 'True'])\n",
    "ax.yaxis.set_ticklabels(['False', 'True'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b29994",
   "metadata": {},
   "source": [
    "# <br />Part 2 - Improved Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723df869",
   "metadata": {},
   "source": [
    "Below is a list and summary of all changes in relation to the code in part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92edf257",
   "metadata": {},
   "source": [
    "### Improvement 1: N-Grams word search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c80364",
   "metadata": {},
   "source": [
    "Added a new parameter <code>ngram_range=(1, 5)</code> to the Vectorizer.<br />\n",
    "This allows the vectorizer to search for the most significant set of words that occur together. The maximum n-gram range had been set to 5<br />Removed the <code>binary=True</code> parameter.<br />Increased the number of features extracted from the text from 750 to 3500, due to an increased number of possible valuable features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0831dc",
   "metadata": {},
   "source": [
    "<pre><code>vectorizer = CountVectorizer(max_features=3500,\n",
    "                             vocabulary=self.vocabulary,\n",
    "                             ngram_range=(1, 5),\n",
    "                             preprocessor=preprocess)</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13dd8d5",
   "metadata": {},
   "source": [
    "### Improvement 2: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f45a3",
   "metadata": {},
   "source": [
    "The equation for Multinomial Naive Bayes is as follows:<br />$$\\hat{c} = argmax_{c∈\\{0, 1\\}}\\biggl[log(p(C=c))+\\sum \\limits_{i=1}^{n}w_{i}\\cdot log(\\frac{N_{ci} +\\alpha}{N_{c}+n\\alpha})\\biggr]$$<br />$$N_{ci}=\\sum\\limits_{w∈T}w_{i}$$<br />$$N_{c}=\\sum\\limits_{i=1}^{n}N_{ci}$$<br />where $N_{ci}$ represents the number of times the feature $i$ appears in a sample of class $c$ in the training set $T$, $N_{c}$ represents the total count of all features for class $c$, and $\\alpha$ is a hyperparameter called Lidstone smoothing (ScikitLearn, 2022).<br />\n",
    "\n",
    "Multinomial Naive Bayes is the best Naive Bayes algorithm for Text Classification and one of the best supervised learning methods for Natural Language Processing. This is because the text data itself is multinomially distributed.<br />\n",
    "Multinomial Naive Bayes is a well-tried and tested method and is a natural improvement over the algorithm used in Part 1 of this assignment in which a a Bernoulli Naive Bayes was used. That is, the previous algorithm did not consider the frequency of the word used and only noted the appearance of that word. Nowadays, Bernoulli models are rarely used and most of the times it is agreed that Multinomial models outperform them. (Abbas et al., 2019)<br />\n",
    "Another option could be to use Complement Naive Bayes algorithm, however it is be best suited for very imbalanced datasets, which is not the case in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31f06b",
   "metadata": {},
   "source": [
    "Steps to implement: Removing the custom <code>naiveBayes</code> and <code>evaluate</code> functions in place of simpler <code>sklearn.naive_bayes.MultinomialNB</code> call.<br />\n",
    "Changing the save options to save the whole <code>MultinomialNB</code> model instead of saving weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee05bd",
   "metadata": {},
   "source": [
    "<pre>In <code>train()</code>:<code>\n",
    "    model = MultinomialNB()\n",
    "    model.fit(self.bag_of_words, self.training_labels)\n",
    "    self.model = model\n",
    "\n",
    "    if save_weights:\n",
    "        np.savez_compressed('resources/weights_and_vocab_p2.npz', model=self.model,\n",
    "                            vocab=self.vocabulary)\n",
    "</code></pre>\n",
    "\n",
    "<pre>In <code>__init__()</code>:<code>\n",
    "    if saved_weights:\n",
    "        \"\"\"Loading saved model and vocabulary for predictions\"\"\"\n",
    "        npz_files = np.load(saved_weights, allow_pickle=True)\n",
    "        self.vocabulary = npz_files['vocab'].tolist()\n",
    "        self.model = npz_files['model'].tolist()\n",
    "    else:\n",
    "        self.vocabulary = None\n",
    "        self.model = None\n",
    "</code></pre>\n",
    "\n",
    "<pre>In <code>predict()</code>:<code>\n",
    "    predictions = self.model.predict(features)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d741aa4",
   "metadata": {},
   "source": [
    "### Improvement 3: TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718897a6",
   "metadata": {},
   "source": [
    "A great additive to the Multinomial Naive Bayes solution that is commonly used is TfIdfVectorizer.<br />\n",
    "In comparison to the classic CountVectorizer, TfIdf offers extra statistical information about the extracted words.<br />\n",
    "In the same journal that was quoted above (Abbas et al., 2019), the authors mention this vectorizer and lists it as a possible enhancement of their work.<br />\n",
    "TfIdf stands for Term Frequency, Inverse Document Frequency. Term Frequency is a simple count of the number of times the word exists in the document - something that CountVectorizer is doing.<br />\n",
    "What CountVectorizer isn't doing is Inverse Document Frequency. TfIdf, on top of regular word counting, calculates the number of documents containing a certain word, which is a divisor to a total number of documents in a dataset. This value is then log'ed and then multiplied by the Term Frequency.<br />\n",
    "It all results in the following equation:\n",
    "$$w_{i,j}=tf_{i,j}\\cdot \\biggl[log(\\frac{N}{df_{i}})\\biggl]$$\n",
    "(Saket, 2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f0828",
   "metadata": {},
   "source": [
    "From the coding perspective, it simpy means that we are swapping out <code>CountVectorizer</code> for <code>TfidfVectorizer</code> from the same <code>sklearn.feature_extraction.text</code> library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0985c",
   "metadata": {},
   "source": [
    "<pre><code>vectorizer = TfidfVectorizer(max_features=3500,\n",
    "                             vocabulary=self.vocabulary,\n",
    "                             ngram_range=(1, 5),\n",
    "                             preprocessor=preprocess)</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed2a59",
   "metadata": {},
   "source": [
    "### Improvement 4: Hyperparameters Cross-Validated Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45871485",
   "metadata": {},
   "source": [
    "Another great tool from sklearn library, that may serve to raise the accuracy of the model is <code>GridSearchCV</code>.<br />\n",
    "Grid search, because the search takes place in the hyperparameter gridspace. In this case, for Multinomial Naive Bayes, the only hyperparameter that can be optimized is $\\alpha$, which stands for Laplace/Lidstone smoothing.<br />\n",
    "The first step is to define the search grid for $\\alpha$. Then we can create the <code>GridSearchCV</code> object and fill out all the parameters. This object will fit the model repeatedly and will save the best performing model into a <code>best_estimator_</code> attribute, which we can write directly to <code>self.model</code> variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b7e43",
   "metadata": {},
   "source": [
    "<pre><code>\n",
    "grid = {'alpha': np.linspace(0.01, 1.01, num=101)}\n",
    "findBestModel = GridSearchCV(\n",
    "    estimator=MultinomialNB(),\n",
    "    param_grid=grid,\n",
    "    scoring='accuracy',\n",
    "    cv=50)\n",
    "findBestModel.fit(self.bag_of_words, self.training_labels)\n",
    "self.model = findBestModel.best_estimator_\n",
    "print(findBestModel.best_params_)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa4877",
   "metadata": {},
   "source": [
    "According to the search above, the optimal value for $\\alpha$ is 0.03. This is the value that I will use from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d006c2",
   "metadata": {},
   "source": [
    "### Improvement 5: Optimised Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f279f",
   "metadata": {},
   "source": [
    "One thing that anyone trying to develop a model will notice is that the results are very dependant on the data split.<br />\n",
    "Some splits are just better and some split are worse. However, they can have a great effect on the model's accuracy, therefore we want to have the best split possible.<br />\n",
    "In order to do that, we could use tools like sklearn's <code>StratifiedShuffleSplit</code>, but for this search, we can simply write a for loop and iterate through the different splits and see which <code>random_state</code> is best.<br />I chose to iterate through 60 different states, as it seemed like an exhaustive enough sample that would not run for too long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b756c",
   "metadata": {},
   "source": [
    "<pre><code>\n",
    "states = []\n",
    "accuracies = []\n",
    "for i in range(1, 60):\n",
    "    train_data, test_data = train_test_split(parsedData, test_size=0.2, random_state=i)\n",
    "\n",
    "    classifier = SentimentClassifier()\n",
    "    classifier.train(train_data, save_weights=False)\n",
    "    _, accuracy = classifier.predict(test_data)\n",
    "    states.append(i)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "ax = sns.scatterplot(states, accuracies)\n",
    "ax.set_xlabel('Random State')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "plt.show()\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400a7ec",
   "metadata": {},
   "source": [
    "After letting this run for a few minutes, the script returns the following plot.\n",
    "\n",
    "<figure>\n",
    "<img src=\"resources/BestStateSearch.png\", width=900>\n",
    "    <figcaption></figcaption>\n",
    "</figure>\n",
    "\n",
    "At first glance, the variance between different splits is noticably high. Considering the fact that we're fighting over every little percent, this is a potentially great optimization boost.<br />\n",
    "The values vary from the lowest scoring seed 20 getting only 73% to the highest scoring seed 28 getting nearly 86%. In general, most of the scores are contained in between 79% and 82%.<br />\n",
    "From now on, we will carry on using the seed 28."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39769dc7",
   "metadata": {},
   "source": [
    "### Improvement 6: Optimised Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd0b29",
   "metadata": {},
   "source": [
    "Another thing which becomes apparent is that not all features that are being extracted from the vectorizer matter. There are features that exist in both classes at similar rates and filtering them away can reduce the vocabulary size and potentially improve the accuracy by a small margin.<br />\n",
    "To do that, a function has been implemented that runs after the model has been trained. It iterates through every feature, calculates a standard deviation and if the result is less than a given threshold, it removes the feature from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2505ce3",
   "metadata": {},
   "source": [
    "<pre><code>\n",
    "def vocab_optimization(self):\n",
    "    featuresToDelete = []\n",
    "\n",
    "    def getWordFromVocab(i):\n",
    "        for word, val in self.vocabulary.items():\n",
    "            if val == i: return word\n",
    "\n",
    "    for i, feature in enumerate(np.transpose(self.model.feature_log_prob_)):\n",
    "        if np.std(feature) < self.threshold:\n",
    "            del self.vocabulary[getWordFromVocab(i)]\n",
    "            featuresToDelete.append(i)\n",
    "\n",
    "    self.bag_of_words = np.delete(self.bag_of_words, featuresToDelete, axis=1)\n",
    "    self.model.feature_count_ = np.delete(self.model.feature_count_, featuresToDelete, axis=1)\n",
    "    self.model.feature_log_prob_ = np.delete(self.model.feature_log_prob_, featuresToDelete, axis=1)\n",
    "    self.vocabulary = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "    self.model.n_features_in_ = len(self.vocabulary)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a4ae1",
   "metadata": {},
   "source": [
    "The question arises, how to elect such parameter that would be the most beneficial to the model.<br />\n",
    "This time, again, I've decided to modify the Classifier in a way that would allow me to pass the threshold in as an attribute and iterate through the parameter grid in search of the most successful one.<br />\n",
    "I chose to design the gridspace in such a way that the plot would allow us to see the bigger picture and present how great of an effect threshold can play, thus the gridspace expanding all the way from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eaeec",
   "metadata": {},
   "source": [
    "<pre><code>\n",
    "acc = []\n",
    "vocab_size = []\n",
    "for threshold in np.concatenate([np.linspace(1, 0.55, 10), np.linspace(0.5, 0.325, 8), np.linspace(0.30, 0, 49)]):\n",
    "    classifier = SentimentClassifier(threshold=threshold)\n",
    "    classifier.train(train_data, save_weights=False)\n",
    "    _, accuracy = classifier.predict(test_data)\n",
    "    acc.append(accuracy)\n",
    "    vocab_size.append(classifier.model.n_features_in_)\n",
    "\n",
    "ax = sns.scatterplot(vocab_size, acc)\n",
    "ax.set_xlabel('Vocabulary Size')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "plt.show()\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29148671",
   "metadata": {},
   "source": [
    "After letting this script run for approximately 15 minutes, here was the result.\n",
    "\n",
    "<figure>\n",
    "<img src=\"resources/LeastVocabSearch.png\", width=900>\n",
    "    <figcaption></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5876e33",
   "metadata": {},
   "source": [
    "Looking at the plot above, we can notice that the number of features we have started with is far from optimal. We could get rid of at least 500 features without decreasing the accuracy (that would correspond to the threshold of 0.05625)<br />\n",
    "However, in between 2500 and 2700 features, we notice a slight improvement in accuracy of approximately 0.4%, which corresponds to the thresholds between 0.09375 and 0.1125. This means that setting our threshold to ~0.1 actually improves model's performance a little bit.<br />\n",
    "In the final version of this code, the value 0.1 is the one that will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b248f",
   "metadata": {},
   "source": [
    "### Improvement 7: Ensemble Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d581d3f",
   "metadata": {},
   "source": [
    "In this section, we'll try to elevate the algorithm one level further and introduce our Multinomial Naive Bayes model to a simple ensemble classifier.<br />\n",
    "Ensemble machine learning methods allow for a creation of a bigger model consisting of multiple conventional models/estimators in such a way that, when put together, they perform better than they normally would on their own.<br />\n",
    "The simplest ensemble method that best illustrates the idea behind it is a Voting Classifier. In it's basic form, it puts the estimators in a democratic environment where they 'vote' for what the output class should be.<br />\n",
    "Another simple ensemble method is a Stacking Classifier. It consists of two layers of estimators. First layer contains a stack of estimators that are being trained in a regular way and they feed their outputs to the second layer. It consists of a single estimator that processes the outputs from the stack and returns the corrected outputs. A common choice for the final estimator is a Logistic Regression classifier, which is already a default option for this method in sklearn library. This is the algorithm that will be used in the final solution.<br /><br />\n",
    "Ensemble methods are numerous and much more refined than the examples above. They are almost always outperforming single algorithms in practice and understanding them is a very important step to becoming a Machine Learning Engineer. The current solution is one of the easiest implementation of ensemble machine learning and this area holds a great potential for future improvement of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd23883",
   "metadata": {},
   "source": [
    "The ensemble model for this solution will be a Stacking Classifier with a single estimator in a stack, which would be our already developed Multinomial Naive Bayes model, and a Logistic Regression algorithm as a final classifier.<br />\n",
    "Logistic Regression hyperparameters have been established via a set of <code>GridSearchCV</code> instances that scanned through a multiple setting variations as shown below. Due to the length of this operation, I will simply summarise the findings and go through the different parameters in short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa01090",
   "metadata": {},
   "source": [
    "<pre><code>\n",
    "grid1 = {\n",
    "    'penalty': ['l2', 'none'],\n",
    "    'C': np.logspace(-3, 3, 20),\n",
    "    'solver': ['lbfgs', 'newton-cg'],\n",
    "}\n",
    "grid2 = {\n",
    "    'penalty': ['elasticnet'],\n",
    "    'C': np.logspace(-3, 3, 20),\n",
    "    'solver': ['saga'],\n",
    "    'l1_ratio': np.linspace(0.001, 1, 21)\n",
    "}\n",
    "grid3 = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-3, 3, 20),\n",
    "    'solver': ['saga']\n",
    "}\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea13bd",
   "metadata": {},
   "source": [
    "The most important parameter that defines other parameters is a solver. It is also the reason why I had to create three different grids for this search. The reason being, solvers are not compatible with some penalty parameters. There are five solvers that are supported in the sklearn library and here is in short how Jeff Hale explains them on his blog.<br />\n",
    " - <code>newton-cg</code> - uses a Hessian Matrix\n",
    " - <code>lbfgs</code> - uses second derivative matrix updates with gradient evaluations\n",
    " - <code>liblinear</code> - uses a coordinate descent algorithm (incompatible with a multinomial models, therefore ignored)\n",
    " - <code>sag</code> - uses a stochastic gradient descent\n",
    " - <code>saga</code> - is an extension of <code>sag</code> (generally better that its predecessor, therefore <code>sag</code> ignored)<br />\n",
    "(Hale J., 2019)\n",
    "\n",
    "The parameter that is most dependent on the solver is a penalty. There are four possible choices for this parameter and they are as follows:<br />\n",
    " - <code>l1</code> - L1 regularization - Incompatible with <code>newton-cg</code> and <code>lbfgs</code>\n",
    " - <code>l2</code> - L2 regularization\n",
    " - <code>elasticnet</code> - A combination of L1 and L2 regularizations. Requires a <code>l1_ratio</code> parameter - Supported only by <code>saga</code>\n",
    " - <code>none</code> - No regularization\n",
    " \n",
    "In short, the main difference between L1 penalty and L2 penalty is the fact that L1 penalizes the sum of absolute values of the weights, whereas L2 penalizes the sum of squares of weights (Pykes, 2021).<br />\n",
    "ElasticNet is a mix of the two above methods. It comes with its own parameter <code>l1_ratio</code>, that takes a float between 0 and 1. When set to 0, penalty is equivalent to L2 and when set to 1, it's equivalent to L1.<br />\n",
    "The results of these three methods are comparable and the differences are small, therefore it's good to perform a search for the most optimal parameters.\n",
    "\n",
    "The last parameter, <code>C</code> stands for inverse regularization strength and it indicates how strongly or weakly our regularization function will react to the distance of the fitted line from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32480510",
   "metadata": {},
   "source": [
    "The results of the following Logistic Regression grid search were used in the Stacking Classifier model. The Accuracy shown is the final result of the whole ensemble model. \n",
    "\n",
    "The best performing parameters from the corresponding grids were:<br /><code>{'penalty': 'l2', 'C': 4.28133, 'solver': 'lbfgs'}</code> - Accuracy ~86.6%<br />\n",
    "<code>{'C': 1.0, 'solver': 'saga', 'l1_ratio': 1.0}</code> - Accuracy ~87.7%<br />\n",
    "<code>{'penalty': 'l1', 'C': 1.0, 'solver': 'saga'}</code> - Accuracy ~87.7%<br />\n",
    "\n",
    "Looking at the test results, we can note that the bottom two are equivalent and they performed around a 1% better than the first model. The results of the third search will be used in the final version of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2494df",
   "metadata": {},
   "source": [
    "In order to implement the ensemble method that was described above, the following modifications must be made:\n",
    "Create another attribute called <code>self.ensemble</code> and use it as a saved and loaded model.\n",
    "<pre><code>\n",
    "if save_weights:\n",
    "    np.savez_compressed('resources/weights_and_vocab_p2.npz', model=self.ensemble,\n",
    "                        vocab=self.vocabulary)\n",
    "</code></pre>\n",
    "<pre><code>\n",
    "if saved_weights:\n",
    "    \"\"\"Loading saved model and vocabulary for predictions\"\"\"\n",
    "    npz_files = np.load(saved_weights, allow_pickle=True)\n",
    "    self.vocabulary = npz_files['vocab'].tolist()\n",
    "    self.ensemble = npz_files['model'].tolist()\n",
    "</code></pre>\n",
    "Then, create the ensemble classifier.\n",
    "<pre><code>\n",
    "\"\"\"Creating a Logistic Regression model with hyperparameters based of the GridSearch conducted previously\"\"\"\n",
    "logistic_regression_model = LogisticRegression(penalty='l1', solver='saga', C=1.0)\n",
    "\n",
    "\"\"\"Creating an fitting an ensemble model with LogisticRegression as a meta-estimator\"\"\"\n",
    "self.ensemble = StackingClassifier(estimators=[('mnb', self.nb_model)],\n",
    "                                   final_estimator=logistic_regression_model,\n",
    "                                   passthrough=True,\n",
    "                                   n_jobs=10).fit(self.bag_of_words, self.training_labels)\n",
    "</code></pre>\n",
    "Note that the attribute <code>self.model</code> containing our Multinomial Naive Bayes model has been renamed to <code>self.nb_model</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe6b45",
   "metadata": {},
   "source": [
    "## Code for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a975c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b4990fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifierP2:\n",
    "    def __init__(self, saved_weights=False):\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "\n",
    "        if saved_weights:\n",
    "            \"\"\"Loading saved model and vocabulary for predictions\"\"\"\n",
    "            npz_files = np.load(saved_weights, allow_pickle=True)\n",
    "            self.vocabulary = npz_files['vocab'].tolist()\n",
    "            self.ensemble = npz_files['model'].tolist()\n",
    "        else:\n",
    "            self.vocabulary = None\n",
    "            self.ensemble = None\n",
    "\n",
    "        self.training_data = None\n",
    "        self.training_labels = None\n",
    "        self.bag_of_words = None\n",
    "        self.nb_model = None\n",
    "        self.vocab_std_threshold = 0.1\n",
    "\n",
    "    @staticmethod\n",
    "    def preProcessing(parsedData):\n",
    "        \"\"\"\n",
    "        Given a dataset containing labels followed by the review comment,\n",
    "        process the data into the right format - label turns into a boolean variable\n",
    "        and a comment is being filtered out of the stop words,\n",
    "        stemmed and parsed into a list of strings.\n",
    "\n",
    "        :param parsedData: a two-dimensional array of input data\n",
    "        :return: labels and processed data\n",
    "        \"\"\"\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        def filterStopWords(toFilter): return [word.lower() for word in toFilter if not word.lower() in stopWords]\n",
    "\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "        car_reviews = np.array(parsedData)\n",
    "\n",
    "        for row in car_reviews:\n",
    "            row[0] = 1 if row[0] == 'Pos' else 0\n",
    "            row[1] = ' '.join([stemmer.stem(word) for word in filterStopWords(word_tokenize(row[1]))])\n",
    "\n",
    "        return car_reviews[:, 0].astype(int), car_reviews[:, 1]\n",
    "\n",
    "    def vectorize(self, data):\n",
    "        \"\"\"\n",
    "        Vectorizing the text samples, saving the vocabulary and sorting it.\n",
    "\n",
    "        :return bag_of_words: fully-processed text data\n",
    "        \"\"\"\n",
    "\n",
    "        def preprocess(text):\n",
    "            \"\"\"Filtering out all the numbers and punctuation\"\"\"\n",
    "            return re.sub(r'\\d+|[.,-?!]', '', text)\n",
    "\n",
    "        vectorizer = TfidfVectorizer(max_features=3500, vocabulary=self.vocabulary,\n",
    "                                     ngram_range=(1, 5), preprocessor=preprocess)\n",
    "\n",
    "        bag_of_words = vectorizer.fit_transform(data).toarray()\n",
    "        if self.vocabulary is None:\n",
    "            self.vocabulary = dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1]))\n",
    "\n",
    "        return bag_of_words\n",
    "\n",
    "    def vocab_optimization(self):\n",
    "        \"\"\"\n",
    "        Optimizing the vocabulary by removing the words that seem\n",
    "        to appear in both negative and positive reviews at similar rates.\n",
    "        \"\"\"\n",
    "        featuresToDelete = []\n",
    "\n",
    "        def getWordFromVocab(i):\n",
    "            for word, val in self.vocabulary.items():\n",
    "                if val == i: return word\n",
    "\n",
    "        for i, feature in enumerate(np.transpose(self.nb_model.feature_log_prob_)):\n",
    "            if np.std(feature) < self.vocab_std_threshold:\n",
    "                del self.vocabulary[getWordFromVocab(i)]\n",
    "                featuresToDelete.append(i)\n",
    "\n",
    "        self.bag_of_words = np.delete(self.bag_of_words, featuresToDelete, axis=1)\n",
    "        self.nb_model.feature_count_ = \\\n",
    "            np.delete(self.nb_model.feature_count_, featuresToDelete, axis=1)\n",
    "        self.nb_model.feature_log_prob_ = \\\n",
    "            np.delete(self.nb_model.feature_log_prob_, featuresToDelete, axis=1)\n",
    "        self.vocabulary = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "        self.nb_model.n_features_in_ = len(self.vocabulary)\n",
    "\n",
    "    def train(self, data, save_weights=False):\n",
    "        \"\"\"\n",
    "        Managing the data preprocessing.\n",
    "        Creating and training the model.\n",
    "\n",
    "        :param data: a two-dimensional array of input data\n",
    "        :param save_weights: flag indicator to save the current weights and vocabulary for future predictions\n",
    "        \"\"\"\n",
    "        self.training_labels, self.training_data = self.preProcessing(data)\n",
    "        self.bag_of_words = self.vectorize(self.training_data)\n",
    "\n",
    "        \"\"\"Creating and fitting the Naive Bayes model with alpha based on the GridSearch conducted previously\"\"\"\n",
    "        self.nb_model = MultinomialNB(alpha=0.03).fit(self.bag_of_words, self.training_labels)\n",
    "\n",
    "        \"\"\"Calling the vocabulary optimization method\"\"\"\n",
    "        self.vocab_optimization()\n",
    "\n",
    "        \"\"\"Creating a Logistic Regression model with hyperparameters based of the GridSearch conducted previously\"\"\"\n",
    "        logistic_regression_model = LogisticRegression(penalty='l1', solver='saga', C=1.0)\n",
    "\n",
    "        \"\"\"Creating an fitting an ensemble model with LogisticRegression as a meta-estimator\"\"\"\n",
    "        self.ensemble = StackingClassifier(estimators=[('mnb', self.nb_model)],\n",
    "                                           final_estimator=logistic_regression_model,\n",
    "                                           passthrough=True,\n",
    "                                           n_jobs=10).fit(self.bag_of_words, self.training_labels)\n",
    "\n",
    "        if save_weights:\n",
    "            np.savez_compressed('resources/weights_and_vocab_p2.npz', model=self.ensemble,\n",
    "                                vocab=self.vocabulary)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Processing the input data and returning a set of predictions given by the trained model.\n",
    "\n",
    "        :param data: a two-dimensional array of input data\n",
    "        :return predictions: a vector of binary predictions\n",
    "        \"\"\"\n",
    "        labels, processed_data = self.preProcessing(data)\n",
    "        features = self.vectorize(processed_data)\n",
    "        predictions = self.ensemble.predict(features)\n",
    "\n",
    "        conf_matrix = confusion_matrix(labels, predictions)\n",
    "        training_set_accuracy = np.mean(np.equal(predictions, labels)) * 100\n",
    "\n",
    "        return conf_matrix, training_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43fbc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = pd.read_csv('resources/car-reviews.csv')\n",
    "train_data_P2, test_data_P2 = train_test_split(parsedData, test_size=0.2, random_state=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91362832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.7256%\n",
      "Training time: 8.4844s\n",
      "Testing time: 0.9531s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell trains the model, saves the weights and tests the algorithm\n",
    "# The operations are timed\n",
    "# Uncomment the lines below to run the training process\n",
    "\n",
    "# classifierP2 = SentimentClassifierP2()\n",
    "\n",
    "# train_start_time = time.process_time()\n",
    "# classifierP2.train(train_data_P2, save_weights=True)\n",
    "# train_end_time = time.process_time()\n",
    "\n",
    "# test_start_time = time.process_time()\n",
    "# conf_matrixP2, accuracyP2 = classifierP2.predict(test_data_P2)\n",
    "# test_end_time = time.process_time()\n",
    "\n",
    "# print(f\"Accuracy: {accuracyP2:.4f}%\\n\"\n",
    "#       f\"Training time: {train_end_time - train_start_time:.4f}s\\n\"\n",
    "#       f\"Testing time: {test_end_time - test_start_time:.4f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48ec4d99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.7256%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the weights and tests the algorithm\n",
    "classifierP2 = SentimentClassifierP2(saved_weights='resources/weights_and_vocab_p2.npz')\n",
    "\n",
    "conf_matrixP2, accuracyP2 = classifierP2.predict(test_data_P2)\n",
    "\n",
    "print(f\"Accuracy: {accuracyP2:.4f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6960989b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Models from part 1 and part 2 side by side\n",
    "train_data_P1, test_data_P1 = train_test_split(parsedData, test_size=0.2, random_state=42)\n",
    "\n",
    "classifierP1 = SentimentClassifier(saved_weights='resources/weights_and_vocab.npz')\n",
    "\n",
    "conf_matrixP1, accuracyP1 = classifierP1.predict(test_data_P1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), dpi=100)\n",
    "sns.heatmap(conf_matrixP1, annot=True, cmap='Reds', fmt='g', ax=ax1, square=True)\n",
    "sns.heatmap(conf_matrixP2, annot=True, cmap='Reds', fmt='g', ax=ax2, square=True)\n",
    "ax1.set_title(f\"Part 1 - Accuracy: {accuracyP1:.4f}%\\n\")\n",
    "ax2.set_title(f\"Part 2 - Accuracy: {accuracyP2:.4f}%\\n\")\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('True Values')\n",
    "    ax.xaxis.set_ticklabels(['False', 'True'])\n",
    "    ax.yaxis.set_ticklabels(['False', 'True'])\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f5340",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As we can see from the side-to-side comparison above, the model from part 2 have achieved an accuracy increase by an approximately ~6.1%.<br />\n",
    "It took a number of parameter optimization, data processing improvement and model design development. In the end, the result is still a small, compact and a powerful model that uses simple measures in order to achieve high accuracy on a relatively small  dataset in sentiment classification.<br />\n",
    "This degree of improvement may have been expected. The first model did not utilise the multinomiality of the features, which led to some data loss. N-Grams have enabled the model to be exposed to the larger sample and the ensemble combination with Logistic Regression improved the final result slightly, by validating the output against an algorithm with a different specification.<br />\n",
    "The greatest and the easiest improvement comes from optimisation. Cross-validated grid search have been a major discovery throughout the course of this assignment and it has contributed greatly to maximising the score. No doubt that any future improvement will require a major redesign of the model and a greater commitment of the resources. I see a great potential in artificial neural networks (ANN). Definitely, a combination of ANNs and the current model is something that I would explore in the first place. I believe that given a larger dataset, this has a potential break the 90% accuracy barrier and looking at the literature for the subject, I don't see the reason why another iteration of the model wouldn't be able to reach 95% accuracy or higher. At this point, such robust algorithm could be ready for commercial use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb4bbf",
   "metadata": {},
   "source": [
    "# References\n",
    "Wikipedia, 2022. Naive Bayes classifier [Online]. Available from: https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "ScikitLearn, 2022. Multinomial Naive Bayes [Online]. Available from: https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes\n",
    "\n",
    "Abbas, M., Memon, K.A., Jamali, A.A., Memon, S. and Ahmed, A. (2019). Multinomial Naive Bayes Classification Model for Sentiment Analysis. IJCSNS International Journal of Computer Science and Network Security, [online] 19(3), pp.62–67. Available at: https://www.researchgate.net/profile/Anees-Ahmed-5/publication/334451164_Multinomial_Naive_Bayes_Classification_Model_for_Sentiment_Analysis/links/5e227e8d92851cafc38c813c/Multinomial-Naive-Bayes-Classification-Model-for-Sentiment-Analysis.pdf\n",
    "\n",
    "Saket, 2020. Count Vectorizer vs TFIDF Vectorizer | Natural Language Processing [Online]. Available from: https://www.linkedin.com/pulse/count-vectorizers-vs-tfidf-natural-language-processing-sheel-saket/\n",
    "\n",
    "Hale J., 2019. Don’t Sweat the Solver Stuff [Online]. Available from: https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "\n",
    "Pykes K., 2021. Fighting Overfitting With L1 or L2 Regularization: Which One Is Better? [Online]. Available from: https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
